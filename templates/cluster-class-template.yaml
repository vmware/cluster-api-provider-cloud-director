apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cni: antrea
  name: ${CLUSTER_NAME}
  namespace: ${TARGET_NAMESPACE}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - ${POD_CIDR} # pod CIDR for the cluster
    services:
      cidrBlocks:
      - ${SERVICE_CIDR} # service CIDR for the cluster
  topology:
    class: ${CLUSTER_CLASS_NAME}
    controlPlane:
      replicas: ${CONTROL_PLANE_MACHINE_COUNT}
    version: ${KUBERNETES_VERSION}
    workers:
      machineDeployments:
      - class: default-worker
        name: worker
        replicas: ${WORKER_MACHINE_COUNT}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: ${CLUSTER_CLASS_NAME}
  namespace: ${TARGET_NAMESPACE}
spec:
  controlPlane:
    ref:
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      kind: KubeadmControlPlaneTemplate
      name: ${CLUSTER_CLASS_NAME}-control-plane-template
    machineInfrastructure:
      ref:
        kind: VCDMachineTemplate
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        name: ${CLUSTER_CLASS_NAME}-control-plane-template
  infrastructure:
    ref:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      kind: VCDClusterTemplate
      name: ${CLUSTER_CLASS_NAME}-cluster
  workers:
    machineDeployments:
      - class: default-worker
        template:
          bootstrap:
            ref:
              apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
              kind: KubeadmConfigTemplate
              name: ${CLUSTER_CLASS_NAME}-worker-template
          infrastructure:
            ref:
              apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
              kind: VCDMachineTemplate
              name: ${CLUSTER_CLASS_NAME}-worker-template
---
apiVersion: v1
kind: Secret
metadata:
  name: capi-user-credentials
  namespace: ${TARGET_NAMESPACE}
type: Opaque
data:
  username: "${VCD_USERNAME_B64}" # B64 encoded username of the VCD persona creating the cluster. If system administrator is the user, please encode 'system/administrator' as the username.
  password: "${VCD_PASSWORD_B64}" # B64 encoded password associated with the user creating the cluster
  refreshToken: "${VCD_REFRESH_TOKEN_B64}" # B64 encoded refresh token of the client registered with VCD for creating clusters. password can be left blank if refresh token is provided
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: VCDClusterTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-cluster
  namespace: ${TARGET_NAMESPACE}
spec:
  template:
    spec:
      site: ${VCD_SITE} # VCD endpoint with the format https://VCD_HOST. No trailing '/'
      org: ${VCD_ORGANIZATION} # VCD organization name where the cluster should be deployed
      ovdc: ${VCD_ORGANIZATION_VDC} # VCD virtual datacenter name where the cluster should be deployed
      ovdcNetwork: ${VCD_ORGANIZATION_VDC_NETWORK} # VCD virtual datacenter network to be used by the cluster
      useAsManagementCluster: false # intent to use the resultant CAPVCD cluster as a management cluster
      userContext:
        secretRef:
          name: capi-user-credentials # name of the secret containing the credentials of the VCD persona creating the cluster
          namespace: ${TARGET_NAMESPACE}  # name of the secret containing the credentials of the VCD persona creating the cluster
      rdeId: ${VCD_RDE_ID}
      loadBalancerConfigSpec:
        vipSubnet: ${VCD_VIP_CIDR} # Virtual IP CIDR for the external network
      proxyConfigSpec:
        httpProxy: ${HTTP_PROXY}
        httpsProxy: ${HTTPS_PROXY}
        noProxy: ${NO_PROXY}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: VCDMachineTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-control-plane-template
  namespace: ${TARGET_NAMESPACE}
spec:
  template:
    spec:
      catalog: ${VCD_CATALOG} # Catalog hosting the TKGm template, which will be used to deploy the control plane VMs
      template: ${VCD_TEMPLATE_NAME} # Name of the template to be used to create (or) upgrade the control plane nodes (this template must be pre-suploaded to the catalog in VCD)
      sizingPolicy: ${VCD_CONTROL_PLANE_SIZING_POLICY} # Sizing policy to be used for the control plane VMs (this must be pre-published on the chosen organization virtual datacenter). If no sizing policy should be used, use "".
      placementPolicy: ${VCD_CONTROL_PLANE_PLACEMENT_POLICY} # Placement policy to be used for worker VMs (this must be pre-published on the chosen organization virtual datacenter)
      storageProfile: "${VCD_CONTROL_PLANE_STORAGE_PROFILE}" # Storage profile for control plane machine if any
      diskSize: ${DISK_SIZE} # Disk size to use for the control plane machine, defaults to 20Gi
      enableNvidiaGPU: false
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlaneTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-control-plane-template
  namespace: ${TARGET_NAMESPACE}
spec:
  template:
    spec:
      kubeadmConfigSpec:
        clusterConfiguration:
          apiServer:
            certSANs:
              - localhost
              - 127.0.0.1
          controllerManager:
            extraArgs:
              enable-hostpath-provisioner: "true"
          dns:
            imageRepository: projects.registry.vmware.com/tkg # image repository to pull the DNS image from
            imageTag: v1.8.4_vmware.9 # DNS image tag associated with the TKGm OVA used. This dns version is associated with TKG OVA using Kubernetes version v1.22.9
          etcd:
            local:
              imageRepository: projects.registry.vmware.com/tkg # image repository to pull the etcd image from
              imageTag: v3.5.4_vmware.2 # etcd image tag associated with the TKGm OVA used. This etcd version is associated with TKG OVA using Kubernetes version v1.22.9.
          imageRepository: projects.registry.vmware.com/tkg # image repository to use for the rest of kubernetes images
        users:
          - name: root
            sshAuthorizedKeys:
              - "${SSH_PUBLIC_KEY}" # ssh public key to log in to the control plane VMs in VCD
        initConfiguration:
          nodeRegistration:
            criSocket: /run/containerd/containerd.sock
            kubeletExtraArgs:
              eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
              cloud-provider: external
        joinConfiguration:
          nodeRegistration:
            criSocket: /run/containerd/containerd.sock
            kubeletExtraArgs:
              eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
              cloud-provider: external
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-worker-template
  namespace: ${TARGET_NAMESPACE}
spec:
  template:
    spec:
      users:
        - name: root
          sshAuthorizedKeys:
            - "${SSH_PUBLIC_KEY}" # ssh public key to log in to the worker VMs in VCD
      joinConfiguration:
        nodeRegistration:
          criSocket: /run/containerd/containerd.sock
          kubeletExtraArgs:
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
            cloud-provider: external
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: VCDMachineTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-worker-template
  namespace: ${TARGET_NAMESPACE}
spec:
  template:
    spec:
      catalog: ${VCD_CATALOG} # Catalog hosting the TKGm template, which will be used to deploy the worker VMs
      template: ${VCD_TEMPLATE_NAME} # Name of the template to be used to create (or) upgrade the control plane nodes (this template must be pre-suploaded to the catalog in VCD)
      sizingPolicy: ${VCD_WORKER_SIZING_POLICY} # Sizing policy to be used for the worker VMs (this must be pre-published on the chosen organization virtual datacenter). If no sizing policy should be used, use "".
      placementPolicy: ${VCD_WORKER_PLACEMENT_POLICY} # Placement policy to be used for worker VMs (this must be pre-published on the chosen organization virtual datacenter)
      storageProfile: "${VCD_WORKER_STORAGE_PROFILE}" # Storage profile for control plane machine if any
      diskSize: ${DISK_SIZE} # Disk size for the worker machine; defaults to 20Gi
      enableNvidiaGPU: false
---