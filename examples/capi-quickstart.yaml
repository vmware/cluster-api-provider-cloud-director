apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: capi-cluster
  namespace: default
  labels:
    cni: antrea
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 100.96.0.0/11 # pod CIDR for the cluster
    serviceDomain: k8s.test
    services:
      cidrBlocks:
      - 100.64.0.0/13 # service CIDR for the cluster
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: capi-cluster-control-plane # name of the KubeadmControlPlane object associated with the cluster.
    namespace: default # kubernetes namespace in which the KubeadmControlPlane object reside. Should be the same namespace as that of the Cluster object
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: VCDCluster
    name: capi-cluster # name of the VCDCluster object associated with the cluster.
    namespace: default # kubernetes namespace in which the VCDCluster object resides. Should be the same namespace as that of the Cluster object
---
apiVersion: v1
kind: Secret
metadata:
  name: capi-user-credentials
  namespace: default
type: Opaque
data:
  username: username # username of the VCD persona creating the cluster. If system administrator is the user, please use 'system/administrator' as the username.
  password: vmware # password associated with the user creating the cluster
  refreshToken: "" # refresh token of the client registered with VCD for creating clusters. username and password can be left blank if refresh token is provided
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VCDCluster
metadata:
  name: capi-cluster
  namespace: default
spec:
  site: https://vcd.vmware.com # VCD endpoint with the format https://VCD_HOST. No trailing '/'
  org: test_org # VCD organization name where the cluster should be deployed
  ovdc: test_orgvdc # VCD virtual datacenter name where the cluster should be deployed
  ovdcNetwork: test_orgvdc_net # VCD virtual datacenter network to be used by the cluster
  parentUid: "urn:vcloud:entity:vmware:capvcdCluster:ParentUUID" # create the CAPVCD cluster from a specific management cluster associated with this UID.
  useAsManagementCluster: false # intent to use the resultant CAPVCD cluster as a management cluster
  userContext:
    secretRef:
      name: capi-user-credentials # name of the secret containing the credentials of the VCD persona creating the cluster
      namespace: default # name of the secret containing the credentials of the VCD persona creating the cluster
  defaultStorageClassOptions: # default storage class configuration options. include this field only if a default storage class needs to be created
    vcdStorageProfileName: "*" # name of the VCD storage profile used to create the storage class
    k8sStorageClassName: "vcd-dev-disk" # name of the storage class in kubernetes
    useDeleteReclaimPolicy: true # if set to true, Delete will be used as the Reclaim policy for the storage class. Else, Retain will be used as the reclaim policy
    fileSystem: "ext4" # file system format for the persistent volumes created from the storage class
  rdeId: "urn:vcloud:entity:vmware:capvcdCluster:UUID" # rdeId if it is already created. If empty, CAPVCD will create one for the cluster.
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VCDMachineTemplate
metadata:
  name: capi-cluster-control-plane
  namespace: default
spec:
  template:
    spec:
      catalog: tkgm-cat # Catalog hosting the TKGm template, which will be used to deploy the control plane VMs
      template: ubuntu-2004-kube-v1.20.8+vmware.1-tkg.1-17589475007677388652 # Name of the template to be used to create (or) upgrade the control plane nodes
      sizingPolicy: tkgm-sizing-policy # Sizing policy to be used for the control plane VMs (this must be pre-published on the chosen organization virtual datacenter)
      placementPolicy: tkgm-placement-policy # Placement policy to be used for worker VMs (this must be pre-published on the chosen organization virtual datacenter)
      storageProfile: "*" # Storage profile to be used for the control plane VMs (this must be pre-published on the chosen organization virtual datacenter)
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: capi-cluster-control-plane
  namespace: default
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: "true"
      dns:
        imageRepository: projects.registry.vmware.com/tkg # image repository to pull the DNS image from
        imageTag: v1.7.0_vmware.12 # DNS image tag associated with the TKGm OVA used. The values must be retrieved from the TKGm ova BOM. Refer to the github documentation for more details
      etcd:
        local:
          imageRepository: projects.registry.vmware.com/tkg # image repository to pull the etcd image from
          imageTag: v3.4.13_vmware.14 # etcd image tag associated with the TKGm OVA used. The values must be retrieved from the TKGm ova BOM. Refer to the github documentation for more details
      imageRepository: projects.registry.vmware.com/tkg # image repository to use for the rest of kubernetes images
    users:
    - name: root
      sshAuthorizedKeys:
      - "ssh-key" # ssh public key to log in to the control plane VMs in VCD
    initConfiguration:
      nodeRegistration:
        criSocket: /run/containerd/containerd.sock
        kubeletExtraArgs:
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
          cloud-provider: external
    joinConfiguration:
      nodeRegistration:
        criSocket: /run/containerd/containerd.sock
        kubeletExtraArgs:
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
          cloud-provider: external
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: VCDMachineTemplate
      name: capi-cluster-control-plane # name of the VCDMachineTemplate object used to deploy control plane VMs. Should be the same name as that of KubeadmControlPlane object
      namespace: default # kubernetes namespace of the VCDMachineTemplate object. Should be the same namespace as that of the Cluster object
  replicas: 1 # desired number of control plane nodes for the cluster
  version: v1.20.8+vmware.1 # Kubernetes version to be used to create (or) upgrade the control plane nodes. The value needs to be retrieved from the respective TKGm ova BOM. Refer to the documentation.
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VCDMachineTemplate
metadata:
  name: capi-cluster-md0
  namespace: default
spec:
  template:
    spec:
      catalog: tkgm-cat # Catalog hosting the TKGm template, which will be used to deploy the worker VMs
      template: ubuntu-2004-kube-v1.20.8+vmware.1-tkg.1-17589475007677388652 # Name of the template to be used to create (or) upgrade the worker nodes
      sizingPolicy: tkgm-sizing-policy # Sizing policy to be used for the worker VMs (this must be pre-published on the chosen organization virtual datacenter)
      placementPolicy: tkgm-placement-policy # Placement policy to be used for worker VMs (this must be pre-published on the chosen organization virtual datacenter)
      storageProfile: "*" # Storage profile to be used for the worker VMs (this must be pre-published on the chosen organization virtual datacenter)
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: capi-cluster-md0
  namespace: default
spec:
  template:
    spec:
      users:
      - name: root
        sshAuthorizedKeys:
        - "ssh-key" # ssh public key to log in to the worker VMs in VCD
      joinConfiguration:
        nodeRegistration:
          criSocket: /run/containerd/containerd.sock
          kubeletExtraArgs:
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
            cloud-provider: external
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: capi-cluster-md0
  namespace: default
spec:
  clusterName: capi-cluster # name of the Cluster object
  replicas: 1 # desired number of worker nodes for the cluster
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: capi-cluster-md0 # name of the KubeadmConfigTemplate object
          namespace: default # kubernetes namespace of the KubeadmConfigTemplate object. Should be the same namespace as that of the Cluster object
      clusterName: capi-cluster # name of the Cluster object
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: VCDMachineTemplate
        name: capi-cluster-md0 # name of the VCDMachineTemplate object used to deploy worker nodes
        namespace: default # kubernetes namespace of the VCDMachineTemplate object used to deploy worker nodes
      version: v1.20.8+vmware.1 # Kubernetes version to be used to create (or) upgrade the worker nodes. The value needs to be retrieved from the respective TKGm ova BOM. Refer to the documentation.
